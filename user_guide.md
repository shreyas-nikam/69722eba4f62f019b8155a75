id: 69722eba4f62f019b8155a75_user_guide
summary: Lab 12: AI Code-Generation Risk Analyzer User Guide
feedback link: https://docs.google.com/forms/d/e/1FAIpQLSfWkOK-in_bMMoHSZfcIvAeO58PAH9wrDqcxnJABHaxiDqhSA/viewform?usp=sf_link
environments: Web
status: Published
# QuLab: Lab 12: AI Code-Generation Risk Analyzer

## Step 1: Introduction to AI Code Security Challenges
Duration: 0:05:00

Welcome to QuLab's Lab 12: AI Code-Generation Risk Analyzer! In today's rapidly evolving software development landscape, AI-powered code generation tools like Copilot, Claude, and various agents are becoming indispensable. While these tools significantly boost developer productivity, they also introduce new security challenges. This lab is designed to help you, in the persona of **Alice, an AppSec Engineer** at InnovateTech Solutions, navigate these challenges.

<aside class="positive">
Understanding these challenges is crucial for building a robust Secure Software Development Lifecycle (SSDLC) that can adapt to AI-assisted coding. <b>This application acts as your virtual assistant</b>, helping you identify and mitigate risks proactively.
</aside>

Alice's primary role is to maintain the security posture of InnovateTech Solutions' applications. With AI-generated code increasingly integrated into critical systems, her team must ensure this code adheres to strict secure coding standards before it ever reaches production. Today, Alice has received a Python API handler that was largely generated by Copilot. Before it proceeds, she needs to perform a thorough security review.

This application simulates Alice's workflow, employing heuristic and Abstract Syntax Tree (AST)-based static analysis, dependency verification, and comprehensive vulnerability reporting.

By the end of this lab, you will be able to:
1.  **Identify common failure modes** in AI-generated code, such as hard-coded secrets and injection vulnerabilities.
2.  **Detect insecure patterns** using static heuristics, understanding how AI might introduce them.
3.  **Flag dependency hallucinations** and potential supply-chain risks introduced by AI's suggestions.
4.  **Define SDLC control gates** specifically tailored for AI-assisted development to ensure security at every stage.
5.  **Produce audit-ready code-gen risk reports**, complete with remediation plans and executive summaries.

Let's begin by understanding how to input the code and context for analysis.

## Step 2: Providing Code and Context for Analysis
Duration: 0:08:00

In this step, Alice sets the stage for a comprehensive security review. The quality and completeness of the input code and context are vital for an accurate and effective analysis. The application starts with default insecure code and dependency information to demonstrate its capabilities.

<aside class="positive">
Providing accurate context is key. Just like a human reviewer benefits from knowing how the code was generated, the analysis tool can provide more relevant findings and recommendations when it understands the origins and review status of the code.
</aside>

### AI-Generated Python Code

This is the primary artifact for analysis. You can paste your Python code directly into the text area or use the default provided. The default code contains several common security pitfalls often introduced by AI code generation, which we will uncover in subsequent steps.

For example, the default code includes:
*   `API_KEY = "sk_prod_12345"`: A hard-coded secret.
*   `subprocess.run(f"echo {command}", shell=True)`: A potential command injection vulnerability.
*   `eval(expr)`: An unsafe use of `eval` that can lead to arbitrary code execution.
*   `query = f"SELECT * FROM users WHERE username = '{username}'"`: A classic SQL injection vulnerability.
*   `pickle.loads(data)`: An insecure deserialization method.

### Dependency Context

Modern applications rely heavily on external libraries. AI-generated code might suggest dependencies that are either non-existent (hallucinations), outdated, or known to have vulnerabilities. This section allows you to provide a `requirements.txt` file and a `dependency_allowlist.json` to vet these external components.

*   **Requirements.txt Content**: This typically lists the Python packages and their versions required by your application.
*   **Dependency Allowlist (JSON)**: This is a critical security control. It defines which packages and which versions are explicitly approved for use within your organization. Any dependency not on this list, or an unapproved version, will be flagged.
    The default allowlist looks like this:
    ```json
    {
        "flask": ["2.1.0", "2.2.0"],
        "requests": ["2.28.1", "2.29.0", "2.30.0"],
        "sqlalchemy": ["1.4.32", "2.0.0"]
    }
    ```
    The default `requirements.txt` includes `unknown-package-malware==1.0.0`, which is deliberately outside this allowlist to demonstrate how the tool flags unauthorized dependencies.

### Generation Context

Understanding how the code was generated helps tailor security expectations and recommendations.

*   **AI Generation Method**: Specify whether the code was generated by Copilot, Claude, or another Agent. Different AI models might have varying tendencies for introducing certain types of vulnerabilities.
*   **Human Review Level**: Indicate how much human review the code has already received ("None", "Partial", "Full"). This helps the analyzer understand the residual risk profile and prioritize findings. A code snippet with "None" human review would typically warrant a more stringent analysis.

**Your Task:**
1.  Ensure you are on the "1. Code & Context Input" page (selected from the sidebar).
2.  Review the default Python code, `requirements.txt` content, and `dependency_allowlist.json`. You can modify them if you wish to experiment, but the defaults are designed to illustrate various findings.
3.  Select "Copilot" as the AI Generation Method and "Partial" as the Human Review Level.
4.  Click the **"Run Analysis"** button at the bottom of the page.

<aside class="negative">
If you see an "Invalid JSON in Dependency Allowlist" error, please double-check your JSON format. Ensure all keys and string values are enclosed in double quotes, and that arrays and objects are correctly bracketed.
</aside>

Once the analysis completes (you'll see a success message), the application will have processed the code for static vulnerabilities, checked dependencies, performed risk scoring, and generated preliminary reports. Now, let's proceed to review the findings.

## Step 3: Reviewing Static Analysis Findings
Duration: 0:07:00

After running the analysis, Alice's next step is to examine the static analysis findings. Static analysis is a powerful technique where the application examines the code *without executing it* to identify potential security vulnerabilities and coding errors. It's like having a meticulous reviewer scrutinize every line of code for insecure patterns.

### Understanding Static Analysis
This application uses heuristic and AST-based static analysis.
*   **Heuristic Analysis**: Looks for patterns or "signatures" of known insecure code. For example, a string containing "password" or "secret" when assigned to a variable might be flagged as a hard-coded secret.
*   **AST-based Analysis**: Builds an Abstract Syntax Tree of the code, which is a structural representation. This allows for more sophisticated checks, like identifying function calls (e.g., `eval()`, `subprocess.run()`, `pickle.loads()`) that are inherently risky, especially when they take user-controlled input.

The goal is to catch common security flaws that AI models might inadvertently introduce, such as:
*   **Hard-coded Secrets**: Credentials, API keys, or sensitive phrases embedded directly in the code.
*   **Injection Flaws**: SQL Injection (where malicious SQL can be inserted) or Command Injection (where malicious operating system commands can be executed).
*   **Insecure Deserialization**: Allowing untrusted data to be converted back into objects, which can lead to remote code execution.
*   **Unsafe Execution**: Using functions like `eval()` or `exec()` with untrusted input.

**Your Task:**
1.  Navigate to the **"2. Static Analysis Findings"** page using the sidebar.
2.  Observe the table of "Identified Code Vulnerabilities". This table lists each finding, its type, severity, confidence, the line number in the code, the problematic code snippet, a description of the vulnerability, and a suggested remediation.
3.  Pay attention to the `risk_type`, `severity`, and `confidence` columns.
    *   **Risk Type**: Categorizes the type of vulnerability (e.g., `Hardcoded Secret`, `Command Injection`).
    *   **Severity**: Indicates the potential impact if the vulnerability is exploited (e.g., Critical, High, Medium, Low).
    *   **Confidence**: Reflects how certain the analyzer is about the finding (e.g., High, Medium, Low). High confidence means it's almost certainly a real issue.

<aside class="console">
Example findings you might observe from the default code:

| risk_type          | severity | confidence | line_num | code_snippet                       | description                                          | remediation                                                                 |
| :-- | :- | : | :- | : | : | :-- |
| Hardcoded Secret   | Critical | High       | 7        | API_KEY = "sk_prod_12345"          | Hardcoded API key detected.                          | Remove hardcoded secrets. Use environment variables or a secure secret manager. |
| SQL Injection      | Critical | High       | 28       | cursor.execute(query)              | Potential SQL Injection via string concatenation.    | Use parameterized queries or ORMs.                                            |
| Command Injection  | High     | High       | 18       | subprocess.run(f"echo {command}",...)| Use of `shell=True` with user input.                 | Avoid `shell=True`. Use `subprocess.run()` with a list of arguments.        |
| Insecure Deserialization | Critical | High | 36     | obj = pickle.loads(data)           | Insecure use of `pickle.loads`.                      | Avoid `pickle` for untrusted data. Use safer formats like JSON.             |
| Unsafe Eval/Exec   | Critical | High       | 22       | return eval(expr)                  | Unsafe use of `eval()`.                              | Avoid `eval()` with untrusted input. Consider an AST-based parser or a sandboxed environment. |
</aside>

This table provides Alice with an immediate overview of critical issues that need attention in the AI-generated code.

## Step 4: Understanding Dependency Risks
Duration: 0:06:00

Beyond the immediate code, Alice must also consider the security of the software supply chain. AI-generated code, while powerful, can sometimes "hallucinate" dependencies or suggest unapproved versions of libraries. This introduces risks such as using malicious packages, vulnerable versions, or components that haven't been vetted by InnovateTech Solutions' security policies.

### Dependency Hallucination and Supply-Chain Risks
*   **Dependency Hallucination**: An AI model might suggest a package that doesn't exist, has a typo, or is a legitimate-sounding but malicious package designed for "typo-squatting" attacks (e.g., `requests-toolbelt` vs. a fake `requests_toolbelt`).
*   **Supply-Chain Risks**: Even legitimate packages can have vulnerabilities (CVEs) or contain malicious code injected by attackers. Controlling which packages and versions are allowed is a fundamental security practice.

This application helps Alice verify declared dependencies against an approved allowlist to identify unknown, unapproved, or suspicious packages.

**Your Task:**
1.  Navigate to the **"3. Dependency Analysis"** page using the sidebar.
2.  Review the "Dependency-Related Issues" table.
3.  Observe how the application identifies the `unknown-package-malware==1.0.0` from the `requirements.txt` as an issue because it's not present in the `dependency_allowlist.json`.

<aside class="console">
Example findings you might observe from the default `requirements.txt`:

| risk_type          | severity | confidence | package                   | version | line_in_requirements_txt | description                                            | remediation                                             |
| :-- | :- | : | : | : | :-- | :-- | : |
| Dependency Hallucination | Critical | High       | unknown-package-malware   | 1.0.0   | 3                        | Package 'unknown-package-malware' not in allowlist. | Use only approved packages. Verify package names and sources. |
</aside>

This analysis is crucial for preventing malicious or vulnerable third-party code from entering the application's ecosystem.

## Step 5: Interpreting the Consolidated Risk Scorecard and Controls
Duration: 0:09:00

After individual reviews, Alice needs a holistic view of all findings, prioritized by risk. This consolidated perspective helps her decide which issues to tackle first and formulate a strategic remediation plan. This section presents an overall risk summary, a detailed risk scorecard, and specific SDLC control recommendations tailored to InnovateTech Solutions' development pipeline.

### Overall Risk Summary
This section provides a high-level overview of the total number of findings, categorized by severity, giving Alice an immediate sense of the code's security posture.

### Risk Score Calculation
To prioritize findings, each vulnerability is assigned a risk score. This score is calculated by combining its **Severity** (how bad it is if exploited) and **Confidence** (how sure we are it's a real issue).

The formula used is:
$$ \text{Risk Score} = \text{Severity Weight} \times \text{Confidence Weight} $$

Where:
*   $\text{Severity Weight}$ is mapped as:
    *   Critical = 4
    *   High = 3
    *   Medium = 2
    *   Low = 1
*   $\text{Confidence Weight}$ is mapped as:
    *   High = 1.0
    *   Medium = 0.75
    *   Low = 0.5

For example, a **Critical** severity finding with **High** confidence would have a Risk Score of $4 \times 1.0 = 4.0$. A **High** severity finding with **Medium** confidence would have a Risk Score of $3 \times 0.75 = 2.25$.

### Consolidated Vulnerabilities
This table combines all findings from static analysis and dependency analysis, sorted by their calculated Risk Score. This ensures that the most impactful and certain vulnerabilities are presented at the top, guiding remediation efforts effectively.

### SDLC Control Recommendations
Based on the identified risks and the generation context (AI method, human review level), the application generates specific recommendations for integrating security controls into InnovateTech Solutions' Software Development Lifecycle. These recommendations are crucial for preventing similar issues in future AI-generated code.

Examples of recommendations might include:
*   Implementing automated secret scanning in CI/CD pipelines.
*   Enforcing dependency allowlists.
*   Mandating security reviews for AI-generated code above a certain risk threshold.
*   Training developers on secure AI prompting.

**Your Task:**
1.  Navigate to the **"4. Consolidated Risk & Controls"** page using the sidebar.
2.  Review the "Overall Risk Summary" to get a snapshot of the findings.
3.  Familiarize yourself with the "Risk Score Calculation" to understand the prioritization logic.
4.  Examine the "Consolidated Vulnerabilities (Sorted by Risk Score)" table. Notice how the vulnerabilities you saw in earlier steps are now combined and ordered.
5.  Read through the "SDLC Control Recommendations" to understand how InnovateTech Solutions can improve its security posture for AI-assisted development.

<aside class="positive">
The consolidated view and SDLC recommendations are Alice's most powerful tools for reporting to management and influencing security policy. <b>They translate raw findings into actionable intelligence.</b>
</aside>

## Step 6: Exporting Analysis Reports
Duration: 0:05:00

The final step for Alice is to generate and export all analysis reports. This is critical for auditability, sharing findings with development teams, and integrating with other security systems or bug tracking platforms. The application ensures all artifacts are properly bundled, making it easy to share a complete evidence package.

### Executive Summary
The Executive Summary provides a high-level, non-technical overview of the analysis, its findings, and the overall risk posture. This is typically geared towards management and stakeholders who need to understand the security implications without diving into technical details.

### Download Analysis Artifacts
All generated reports and evidence for the analysis session are available for download. This includes the individual JSON and Markdown files, as well as a convenient ZIP archive containing everything. The ZIP archive is particularly useful for attaching to tickets, archiving, or sharing a complete audit package.

The generated artifacts include:
*   `code_findings.json`: Detailed static analysis results.
*   `dependency_findings.json`: Details on dependency-related issues.
*   `risk_scorecard.json`: The raw data for the risk summary and individual risk scores.
*   `sdlc_control_recommendations.md`: The markdown content for SDLC controls.
*   `session12_executive_summary.md`: The markdown content of the executive summary.
*   `config_snapshot.json`: A snapshot of the input configuration (code hashes, generation context) for reproducibility and auditing.
*   `evidence_manifest.json`: A manifest detailing all generated files, including their SHA256 hashes, to ensure integrity and prove what was analyzed.

**Your Task:**
1.  Navigate to the **"5. Report Export"** page using the sidebar.
2.  Read the "Executive Summary" to see how the findings are summarized for non-technical audiences.
3.  Click the **"Download All Reports (Session_[ID].zip)"** button to download the complete archive of all analysis artifacts.
4.  Optionally, download individual report files to explore their content further.

<aside class="positive">
Downloading the full ZIP archive ensures you have a complete, timestamped, and hashed record of the analysis, which is invaluable for <b>compliance, auditing, and post-incident review.</b>
</aside>

Congratulations! You have successfully completed the QuLab Lab 12: AI Code-Generation Risk Analyzer. You've walked through the process of analyzing AI-generated code for security vulnerabilities, identified dependency risks, interpreted a consolidated risk scorecard, and exported comprehensive reports. This experience will empower you to better secure your applications in an era of AI-accelerated development.
